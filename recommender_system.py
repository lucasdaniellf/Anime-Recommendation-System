# -*- coding: utf-8 -*-
"""Recommender System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QF9GIH55LtnPmrb7KX_zMruFcVXhmI5h

# Data Preparation
"""

pip install scikit-surprise

from google.colab import drive
from google.colab import files
import pandas as pd
import numpy as np
import surprise as sp
import matplotlib.pyplot as plt
import json
import csv

"""Since the size of the data file with the scores is over 2.0 GB, I uploaded the file in my google drive and loaded it from there. The other two files were uploaded directly from my computer."""

drive.mount('/content/gdrive')

uploaded = files.upload()

with open ('/content/anime_cleaned.csv') as anime_cleaned:
  DF_anime = pd.read_csv(anime_cleaned)
with open ('/content/users_cleaned.csv') as users_cleaned:
  DF_users = pd.read_csv(users_cleaned)
with open('/content/gdrive/My Drive/animelists_cleaned.csv') as animelist_cleaned:
  DF_ratings = pd.read_csv(animelist_cleaned)

# I was going to keep movies from the recommendation system because, in the content-based section, 
# the movies of a series would all take top spots when generating a similarity list for a specific series.
# Although this is to be expected, I wanted to get recommendations of different series, and not movies and special of
# the series I just searched. Ultimately I left the movies as a possibility for recommendation so I could see if the 
# system was returning a expected result and working as intended.

#DF_anime = DF_anime[DF_anime.type != 'Movie']
#DF_anime.type.unique()

DF_anime = DF_anime[['anime_id', 'title', 'title_english', 'genre', 'score', 'scored_by']]

DF_users = DF_users[['username', 'user_id', 'user_watching', 'user_completed']]

DF_ratings = DF_ratings[['username', 'anime_id', 'my_score']]

DF_anime.sort_values(by='scored_by', ascending=False).head()

DF_anime.describe()

DF_anime.isna().any()

DF_anime[DF_anime.genre.isna()]

DF_anime = DF_anime[DF_anime.genre.notna()]
DF_anime.sort_values(by='scored_by', ascending=False, inplace=True)
DF_anime.reset_index(drop=True, inplace=True)
DF_anime.head()

"""# MAKING A LIST WITH ALL THE POSSIBLE GENRES FOR THE CONTENT-BASED RECOMMENDER"""

anime_sample = DF_anime.sample(n=100, random_state=0)

genres = []

for genre in anime_sample['genre']:
 genre = genre.split(', ')
 for string in genre:
   if string in genres:
     pass
   else:
     genres.append(string)
genres.sort()

DF_anime_dict = {}

for i in range(len(DF_anime)):
  list_of_genres = {}
  DF_anime_genre = DF_anime.iloc[i].genre.split(', ')
  
  for genre in genres:
    if genre in DF_anime_genre:
      list_of_genres[genre] = 1
    elif genre not in DF_anime_genre:
      list_of_genres[genre] = 0
    DF_anime_dict[DF_anime.iloc[i].title] = list_of_genres
  
DF_Anime_matrix = pd.DataFrame.from_dict(DF_anime_dict, orient='index')

DF_Anime_matrix.head()

"""#ADDING ANIMES TO THE USER'S LIST"""

# Here it is possible to add an anime from the database to the User's Personal List. It saves in the drive, but the path
# can be changed.

checking_anime = False
anime = input('>>> Inform the anime you want to add to your list: ')

if (DF_anime['title'].str.lower() == anime.strip().lower()).any():
  anime_new = DF_anime[DF_anime['title'].str.lower() == anime.strip().lower()]
  checking_anime = True

elif (DF_anime['title_english'].str.lower() == anime.strip().lower()).any():
  anime_new = DF_anime[DF_anime['title_english'].str.lower() == anime.strip().lower()]
  checking_anime = True

if checking_anime is True:
  try:
    User_data = User_data.append(anime_new)
      
    User_data.drop_duplicates(inplace=True)
    User_data.sort_values(by='title', inplace=True)
    User_data.reset_index(drop = True, inplace=True)
    
    User_data.to_csv('/content/gdrive/My Drive/User_data_2.csv', mode='w')
  except:
    User_data = anime_new

    User_data.reset_index(drop= True, inplace=True)
    User_data.insert(4, 'user_rating', np.nan)
    User_data.to_csv('/content/gdrive/My Drive/User_data_2.csv', mode='w')
  finally:
    print('Anime added successfully!')

# I put this one here just so I don't have to fill the user's list every time

User_data = pd.read_csv('/content/gdrive/My Drive/User_data_2.csv', index_col=0)
User_data

"""# Creating the User_Genre matrix used to calculate the User Preferences"""

User_genre_dict = {}

for i in range(len(User_data)):
  list_of_genres = {}
  anime_user_genre = User_data.iloc[i].genre.split(', ')
  for genre in genres:
    if genre in anime_user_genre:
      list_of_genres[genre] = 1
    elif genre not in anime_user_genre:
      list_of_genres[genre] = 0
    User_genre_dict[User_data.iloc[i].title] = list_of_genres
    
DF_User_preference = pd.DataFrame.from_dict(User_genre_dict, orient='index')
DF_User_preference

"""# Inform the user's rating of an anime"""

anime = input('>>> Inform the anime you want to update the rating: ')
checking_anime = False

for i in range(len(User_data)):
    if anime.strip().lower() == str(User_data.iloc[i].title).lower():
      checking_anime = True
    elif anime.strip().lower() == str(User_data.iloc[i].title_english).lower():
      checking_anime = True

    if checking_anime is True:
      anime = User_data.iloc[i].title
      try:
        rating = int(input('>>> Inform your rating of the anime from 0-10: '))
      except ValueError:
        print('Invalid rating')
        break
      else:
        if rating < 0:
          rating = 0
        elif rating > 10:
          rating = 10
        
        User_data.at[i, 'user_rating'] = rating

        User_anime_rating = User_data.user_rating.to_numpy()

        User_anime_rating = np.array([User_anime_rating]).transpose()

        DF_User_preference_weight = DF_User_preference*User_anime_rating

        break

User_data.to_csv('/content/gdrive/My Drive/User_data_2.csv', mode='w')
DF_User_preference_weight

# As before, this cell is here so we can have the DF_User_preference_weight without
# having to run the cell above.
 
User_anime_rating = User_data.user_rating.values
User_anime_rating = np.array([User_anime_rating]).transpose()
DF_User_preference_weight = DF_User_preference*User_anime_rating

DF_User_preference_weight

"""The User preference matrix:"""

DF_User_matrix = (DF_User_preference_weight.sum(axis=0)/(((DF_User_preference_weight.sum())).sum())).to_frame().transpose()
DF_User_matrix.rename({0:'weights'}, axis='index', inplace=True)
DF_User_matrix

Recommend = (DF_Anime_matrix*DF_User_matrix.values).sum(axis=1)/((np.linalg.norm(DF_Anime_matrix, axis=1))*(np.linalg.norm(DF_User_matrix, axis=1)))

for i in range(len(User_data)):
  if User_data.iloc[i].title in Recommend.index.to_list():
    Recommend.drop(index=User_data.iloc[i].title, inplace=True)


Recommend.sort_values(ascending=False).head(n=20)

"""# COLLABORATIVE FILTERING

This is the section where I start the collaborative filtering part of the recommender. I wrote the whole algorithms instead of using the surprise library just so I could learn more about the subject.
I used the svd method and another one that is basically a svd with the s matrix of 1's, and it runs with gradient descent to find the latent factors of users and animes.
"""

# Was going to left users with less than 100 iterations out of analysis. I do something different, but similar, later on.

#DF_users = DF_users[(DF_users['user_completed'] + DF_users['user_watching']) > 100]
#DF_users.head()

DF_users.shape

"""Taking from the score list the animes removed previously (the ones without genres)"""

DF_ratings_filtered = DF_ratings[DF_ratings.anime_id.isin(DF_anime.anime_id.to_list())]
DF_ratings_filtered.shape

"""Number of animes in our data"""

len(DF_ratings_filtered.anime_id.unique())

"""Getting how many animes each user have rated"""

DF_ratings_user = DF_ratings_filtered.groupby(by=['username']).count()
DF_ratings_user.rename(columns={'my_score':'nº of ratings'}, inplace=True)


DF_ratings_user = DF_ratings_user.drop(columns=['anime_id'], axis=1)
DF_ratings_user.reset_index(inplace=True)
DF_ratings_user.sort_values(by=['nº of ratings'], ascending=True).head()

DF_ratings_user.shape, DF_users.shape

"""I'm not going to use users who have rated less than 10 animes for the model."""

DF_ratings_user = DF_ratings_user[DF_ratings_user['nº of ratings'] >=10]
DF_ratings_user.shape

"""For performance's sake I'll just create a model using 1500 users chosen randomly"""

Users_sample = DF_ratings_user.sample(n=1500, random_state=0)

DF_ratings_sample = DF_ratings_filtered[DF_ratings_filtered.username.isin(Users_sample.username.to_list())]

DF_ratings_sample.shape

## JUST CHECKING ##


x = DF_ratings_sample.groupby(by=['username']).count()
x.rename(columns={'my_score':'nº of ratings'}, inplace=True)


x = x.drop(columns=['anime_id'], axis=1)
x.reset_index(inplace=True)
x.sort_values(by=['nº of ratings'], ascending=True).head()

"""Number of ratings each anime have in the sample of 1500 users"""

DF_ratings_anime = DF_ratings_sample.groupby(by=['anime_id']).count()
DF_ratings_anime.rename(columns={'my_score':'nº of ratings'}, inplace=True)


DF_ratings_anime = DF_ratings_anime.drop(columns=['username'], axis=1)
DF_ratings_anime.reset_index(inplace=True)
DF_ratings_anime.sort_values(by=['nº of ratings'], ascending=True).head()

DF_ratings_anime.shape

"""Dropping the anime with less than 10 ratings"""

DF_ratings_anime = DF_ratings_anime[DF_ratings_anime['nº of ratings'] > 10]

## JUST CHECKING IF OUR USERS LIST NOW HAS SOMEONE WITH LESS THAN 10 REVIEWS
DF_ratings_final = DF_ratings_sample[DF_ratings_sample.anime_id.isin(DF_ratings_anime.anime_id.to_list())]


a = DF_ratings_final.groupby(by=['username']).count()
a.rename(columns={'my_score':'nº of ratings'}, inplace=True)


a = a.drop(columns=['anime_id'], axis=1)
a.reset_index(inplace=True)
a.sort_values(by=['nº of ratings'], ascending=True).head()

DF_ratings_anime.shape

"""I'll use DF_ratings_final for the analysis and model building"""

DF_anime_to_merge = DF_anime[['anime_id', 'title']]

DF_ratings_final = DF_ratings_final.merge(DF_anime_to_merge, on='anime_id')
DF_ratings_final.shape

DF_ratings_final.drop(columns=['anime_id'], inplace=True)
Pivot_Data = DF_ratings_final.pivot_table(index='title', columns='username')

"""Pivot_Data is the the table with the ratings each user gave to each anime"""

Pivot_Data

User_mean = Pivot_Data.mean(axis=0).values
Pivot_Data_Normalized = Pivot_Data - User_mean

"""Number of valid ratings in the pivot_data table"""

Pivot_Data_Normalized.count().sum()

"""# Gradient Descent Approach

This is the step where we train our model. Since it uses Gradient Descent, it converges really slowly. I defined the program to iterate 8000 times or until the Cost Function decreases less than 0.0001, whichever comes first.

The algorithm initializes random small values for the Anime Features and Users Coefficients and update then each step.

This is a model-based collaborative filtering recommender system, it applies the low-rank matrix factorization in the pivot_data matrix and to generate coefficients that describe the users preferences and the animes characteristics. And that's the reason I chose not to use animes with less than 10 ratings and users with less than 10 animes rated, so they would not affect our model with biased samples.

Pivot Table Normalized
"""

J_list = []
lambda_reg = 0.0007
alpha_f = 0.0005
alpha_c = 0.00025

Users_Coefficients = np.random.rand(10,1500)
Anime_Features = np.random.rand(4385,10)
J_Cost_temp = 10**20

for i in range(15000):

  Rating_Predict = Anime_Features.dot(Users_Coefficients)
  

  Reg_Term = (lambda_reg/2)*((((Users_Coefficients**2).sum(axis=1)).sum()) + (((Anime_Features**2).sum(axis=1)).sum()))
  J_Cost = (1/2)*(((Rating_Predict - Pivot_Data_Normalized)**2).sum(skipna=True)).sum(skipna=True) + Reg_Term
  
  if (abs(J_Cost - J_Cost_temp) < 0.8):
    break

  J_Cost_temp = J_Cost
  print(i, J_Cost)
  
  J_list.append(J_Cost)

  Anime_Features_temp = alpha_f*(((Rating_Predict - Pivot_Data_Normalized).fillna(value=0)).dot(Users_Coefficients.transpose()))
  Anime_Features = (1-lambda_reg*alpha_c)*Anime_Features - Anime_Features_temp 
 
  Users_Coefficients_temp = alpha_c*((((Rating_Predict - Pivot_Data_Normalized).fillna(value=0)).transpose()).dot(Anime_Features))
  Users_Coefficients = (1-lambda_reg*alpha_c)*Users_Coefficients - Users_Coefficients_temp.transpose()

Users_Coefficients.to_csv(r'/content/gdrive/My Drive/Users_Coefficients_Ofc.csv')
Anime_Features.to_csv(r'/content/gdrive/My Drive/Anime_Features_Ofc.csv')

"""# SVD APPROACH"""

from scipy.sparse.linalg import svds
from scipy.sparse import csr_matrix

A = csr_matrix(Pivot_Data_Normalized.T.fillna(0))
U,s,V = svds(A, k=10)

s_matrix = np.zeros([10,10])
for i in range(len(s)):
  s_matrix[i][i] = s[i]

user_coefficients_svd = pd.DataFrame(U, index = Pivot_Data.my_score.columns.values.tolist())
user_coefficients_svd.head()

anime_features_svd = pd.DataFrame(V.T, index = Pivot_Data.index.values.tolist())
anime_features_svd.head()

user_coefficients_svd.to_csv(r'/content/gdrive/My Drive/user_coefficients_svd.csv')
anime_features_svd.to_csv(r'/content/gdrive/My Drive/anime_features_svd.csv')

Prediction_Matrix_array = np.matmul(np.matmul(U, s_matrix), V)
Prediction_Matrix = pd.DataFrame(Prediction_Matrix_array, index = Pivot_Data.my_score.columns.values.tolist(), columns = Pivot_Data.index.values.tolist())
Prediction_Matrix.T.head()

col = list(Pivot_Data_Normalized.my_score.columns.values)
ind = list(Pivot_Data_Normalized.index)
data = Pivot_Data_Normalized.values
Rating_Real = pd.DataFrame(data, index=ind, columns=col)
Rating_Real.count().sum()

"""Scaling to 0~1 for easier analysis"""

Rating_Real_Scaled = (Rating_Real - Rating_Real.min())/(Rating_Real.max() - Rating_Real.min())

Prediction_Matrix_Scaled = (Prediction_Matrix.T - Prediction_Matrix.min(axis=1))/(Prediction_Matrix.max(axis=1) - Prediction_Matrix.min(axis=1))

Prediction_Matrix_Sparsed = Prediction_Matrix_Scaled + Rating_Real - Rating_Real

Rating_Couple = Rating_Real_Scaled.T.values.copy()
Rating_Real_NP = Rating_Real_Scaled.fillna(value=0).T.values.copy()
Rating_Matrix_NP = Prediction_Matrix_Scaled.T.values.copy()

Rating_Couple = Rating_Couple.tolist()

for i in range(len(Rating_Real_NP)):
  for j in range(len(Rating_Real_NP[i])):
    Rating_Couple[i][j] = (Rating_Real_NP[i][j], Rating_Matrix_NP[i][j])

"""Verifying the top 10 rated animes of each user and seeing if the system recommended them"""

recommended = 0
relevant = 0
true_pos = 0

true_pos_list = []
rec_list = []
rel_list = []

for i in range(len(Rating_Couple)):
  Rating_Couple[i].sort(key=lambda x: x[0], reverse=True)
  K_Relevant = Rating_Couple[i][0:10]

  true_pos_i = 0
  rec_i = 0
  rel_i = 0

  for couple in K_Relevant:
    if couple[0] >= 0.5:
      rel_i = rel_i + 1
      relevant = relevant + 1
    if couple[1] >= 0.5:
      rec_i = rec_i + 1
      recommended = recommended + 1
    if couple[0] >= 0.5 and couple[1] >= 0.5:
      true_pos_i = true_pos_i + 1
      true_pos = true_pos + 1 
    
  true_pos_list.append(true_pos_i)
  rec_list.append(rec_i)
  rel_list.append(rel_i)

true_pos, recommended, relevant

"""We see that the system got right almost everything that was recommended, but it failed to recommend a lot of animes in the users' top 10. If we lower the threshold we increase the recall value but we lose precision at the same time. Work with the threshold to get a good f1 score is one option to get a better performance, besides trying to optimize the algorithm."""

precision_K = true_pos/recommended
recall_K = true_pos/relevant

precision_K, recall_K

"""For each person"""

precision_i = np.zeros(1500)
recalls_i = np.zeros(1500)

for i in range(len(true_pos_list)):
  precision_i[i] = true_pos_list[i]/rec_list[i] if rec_list[i] != 0 else 1
  recalls_i[i] = true_pos_list[i]/rel_list[i] if rel_list[i] != 0 else 1

prec_mean = sum(precision_i)/len(precision_i)
rec_mean = sum(recalls_i)/len(recalls_i)

prec_mean, rec_mean

"""For all Predictions:"""

true_pos = 0
relevant = 0
recommended = 0

for i in range(len(Prediction_Matrix_Sparsed.values)):
  for j in range(len(Prediction_Matrix_Sparsed.values[i])):
    if Prediction_Matrix_Sparsed.values[i][j] >= 0.5 and Rating_Real_Scaled.values[i][j] >= 0.5:
      true_pos = true_pos + 1
   
    if Rating_Real_Scaled.values[i][j] >= 0.5:
      relevant = relevant+1
    
    if Prediction_Matrix_Sparsed.values[i][j] >= 0.5:
      recommended = recommended + 1

true_pos, relevant, recommended

true_pos/recommended, true_pos/relevant

"""# Analyzing the Coefficients from Downloaded File

To avoid having to run the training algorithm of the gradient descent section everytime I opened this file, I uploaded the anime features and users coefficients in my drive. I'll call them here so I can use them.
"""

Anime_Features_Down = pd.read_csv('/content/gdrive/My Drive/Anime_Features_Ofc.csv')
Anime_Features_Down.set_index('title', drop=True, inplace=True)
Anime_Features_Down.head()

Users_Coeff_Down = pd.read_csv('/content/gdrive/My Drive/Users_Coefficients_Ofc.csv', header=1)
Users_Coeff_Down.rename(columns={'username': '' }, inplace=True)
Users_Coeff_Down.set_index('', drop=True, inplace=True)
Users_Coeff_Down.head()

Rating_Matrix = np.matmul(Anime_Features_Down.values, Users_Coeff_Down.values)
Rating_Matrix = pd.DataFrame(data=Rating_Matrix, index=Anime_Features_Down.index.to_list(), columns = Users_Coeff_Down.columns.to_list())
Rating_Matrix.head()

col = list(Pivot_Data_Normalized.my_score.columns.values)
ind = list(Pivot_Data_Normalized.index)
data = Pivot_Data_Normalized.values
Rating_Real = pd.DataFrame(data, index=ind, columns=col)
Rating_Real.count().sum()

"""Scaling to 0~1 for easier analysis"""

Rating_Real_Scaled = (Rating_Real - Rating_Real.min())/(Rating_Real.max() - Rating_Real.min())

Rating_Matrix_Scaled = (Rating_Matrix - Rating_Matrix.min())/(Rating_Matrix.max() - Rating_Matrix.min())

Rating_Matrix_Sparsed = Rating_Matrix_Scaled + Rating_Real - Rating_Real

Rating_Couple = Rating_Real_Scaled.T.values.copy()
Rating_Real_NP = Rating_Real_Scaled.fillna(value=0).T.values.copy()
Rating_Matrix_NP = Rating_Matrix_Sparsed.fillna(value=0).T.values.copy()

Rating_Couple = Rating_Couple.tolist()

for i in range(len(Rating_Real_NP)):
  for j in range(len(Rating_Real_NP[i])):
    Rating_Couple[i][j] = (Rating_Real_NP[i][j], Rating_Matrix_NP[i][j])

recommended = 0
relevant = 0
true_pos = 0

true_pos_list = []
rec_list = []
rel_list = []

for i in range(len(Rating_Couple)):
  Rating_Couple[i].sort(key=lambda x: x[0], reverse=True)
  K_Relevant = Rating_Couple[i][0:10]

  true_pos_i = 0
  rec_i = 0
  rel_i = 0

  for couple in K_Relevant:
    if couple[0] >= 0.5:
      rel_i = rel_i + 1
      relevant = relevant + 1
    if couple[1] >= 0.5:
      rec_i = rec_i + 1
      recommended = recommended + 1
    if couple[0] >= 0.5 and couple[1] >= 0.5:
      true_pos_i = true_pos_i + 1
      true_pos = true_pos + 1 
    
  true_pos_list.append(true_pos_i)
  rec_list.append(rec_i)
  rel_list.append(rel_i)

true_pos, recommended, relevant

precision_K = true_pos/recommended
recall_K = true_pos/relevant

precision_K, recall_K

"""For each person:"""

precision_i = np.zeros(1500)
recalls_i = np.zeros(1500)
for i in range(len(true_pos_list)):
  precision_i[i] = true_pos_list[i]/rec_list[i] if rec_list[i] != 0 else 1
  recalls_i[i] = true_pos_list[i]/rel_list[i] if rel_list[i] != 0 else 1

prec_mean = sum(precision_i)/len(precision_i)
rec_mean = sum(recalls_i)/len(recalls_i)

prec_mean, rec_mean

"""Precision and Recall for all recommendations"""

true_pos = 0
relevant = 0
recommended = 0

for i in range(len(Rating_Matrix_Sparsed.values)):
  for j in range(len(Rating_Matrix_Sparsed.values[i])):
    if Rating_Matrix_Sparsed.values[i][j] >= 0.6 and Rating_Real_Scaled.values[i][j] >= 0.6:
      true_pos = true_pos + 1
   
    if Rating_Real_Scaled.values[i][j] >= 0.6:
      relevant = relevant+1
    
    if Rating_Matrix_Sparsed.values[i][j] >= 0.6 :
      recommended = recommended + 1

true_pos, relevant, recommended

true_pos/recommended, true_pos/relevant

"""# Getting the recommendation for a specific anime"""

DF_anime[DF_anime.title == 'Chobits']

"""From the Gradient Descent Algorithm"""

anime_val =  'Cowboy Bebop'
((Anime_Features_Down.loc[anime_val] * Anime_Features_Down).sum(axis=1)/(np.linalg.norm(Anime_Features_Down.loc[anime_val])* np.linalg.norm(Anime_Features_Down, axis=1))).sort_values(ascending=False).head(n=20)

"""From the svd algorithm"""

anime_val =  'Cowboy Bebop'
((anime_features_svd.loc[anime_val] * anime_features_svd).sum(axis=1)/(np.linalg.norm(anime_features_svd.loc[anime_val])* np.linalg.norm(anime_features_svd, axis=1))).sort_values(ascending=False).head(n=20)

"""# TEST SET FOR THE MODEL

I'll use the anime features and s_matrix I obtained from the svd section from now on, With them I'll calculate the coefficients of the users in the test set and then analyze how well the model performed.

Getting only users who are not in the training set
"""

DF_ratings_user_test = DF_ratings_user[~DF_ratings_user.username.isin(Users_sample.username.to_list())]

Users_sample_test = DF_ratings_user_test.sample(n=300, random_state=2)

DF_ratings_test = DF_ratings_filtered[DF_ratings_filtered.username.isin(Users_sample_test.username.to_list())]

DF_ratings_test.shape

## JUST CHECKING ##
y = DF_ratings_test.groupby(by=['username']).count()
y.rename(columns={'my_score':'nº of ratings'}, inplace=True)


y = y.drop(columns=['anime_id'], axis=1)
y.reset_index(inplace=True)
y.sort_values(by=['nº of ratings'], ascending=True).head()

"""Number of ratings per anime in the sample of 300 users"""

DF_ratings_test_anime = DF_ratings_test.groupby(by=['anime_id']).count()
DF_ratings_test_anime.rename(columns={'my_score':'nº of ratings'}, inplace=True)


DF_ratings_test_anime = DF_ratings_test_anime.drop(columns=['username'], axis=1)
DF_ratings_test_anime.reset_index(inplace=True)
DF_ratings_test_anime.sort_values(by=['nº of ratings'], ascending=True).head()

DF_ratings_test_anime.shape

"""Selecting only the anime that are in the training set"""

DF_Scores_ID = DF_ratings_final.merge(DF_anime_to_merge, on='title')
DF_Scores_ID.drop(columns=['title'], inplace=True)
DF_Scores_ID.shape

DF_Scores_ID.head()

DF_ratings_test_anime = DF_ratings_test_anime[DF_ratings_test_anime.anime_id.isin(DF_Scores_ID.anime_id.to_list())]
DF_ratings_test_anime.shape

## Removing the ratings of the removed animes from the ratings list
DF_ratings_test = DF_ratings_test[
DF_ratings_test.anime_id.isin(DF_ratings_test_anime.anime_id.to_list())]


a = DF_ratings_test.groupby(by=['username']).count()
a.rename(columns={'my_score':'nº of ratings'}, inplace=True)

a = a.drop(columns=['anime_id'], axis=1)

a.reset_index(inplace=True)
a.sort_values(by=['nº of ratings'], ascending=True).head()

DF_ratings_test_anime.shape

DF_ratings_test = DF_ratings_test.merge(DF_anime_to_merge, on='anime_id')
DF_ratings_test.shape

DF_ratings_test.drop(columns=['anime_id'], inplace=True)
Pivot_Data_test = DF_ratings_test.pivot_table(index='title', columns='username')

User_mean_test = Pivot_Data_test.mean(axis=0).values
Pivot_Data_Normalized_test = Pivot_Data_test - User_mean_test

"""Getting the features from the animes that were rated by our users in the test set (I'll use the features obtained from the svd approach)"""

Anime_Features_svd_test = anime_features_svd[anime_features_svd.index.isin(Pivot_Data_test.index)]

"""Using the anime features obtained previously, I run a gradient descent algorithm to obtain the coefficients of our users in the test set."""

J_list = []
J_Cost_temp = 10**20
Users_Coeff_test = np.random.rand(10,300)
m = len(Anime_Features_svd_test)
i = 0

# Already taking into account the s_matrix
Anime_Features_Test = Anime_Features_svd_test.values @ s_matrix

lambda_reg = 0.01
alpha_c = 0.05

while True:

  Predict_test = Anime_Features_Test @ Users_Coeff_test

  Reg_Term = (lambda_reg/(2*m))*(((Users_Coeff_test**2).sum(axis=1)).sum())
 
  J_Cost = (1/(2*m))*np.nansum((Predict_test - Pivot_Data_Normalized_test.values)**2)+ Reg_Term
  
  if abs(J_Cost - J_Cost_temp) < 0.0005:
    break

  J_Cost_temp = J_Cost
  
  #print(i, J_Cost)
  
  i = i+1
  J_list.append(J_Cost)
 
  Users_Coeff_test_temp = (alpha_c/m)*(((Predict_test - Pivot_Data_Normalized_test).fillna(value=0).T.values) @ Anime_Features_Test)

  Users_Coeff_test = (1-lambda_reg*alpha_c/m)*Users_Coeff_test - Users_Coeff_test_temp.T

Users_Coeff_Test_DF = pd.DataFrame(Users_Coeff_test.T, index = Pivot_Data_test.my_score.columns.values.tolist())
Users_Coeff_Test_DF.head()

Anime_Features_Test_DF = pd.DataFrame(Anime_Features_Test, index = Pivot_Data_test.index.values.tolist())
Anime_Features_Test_DF.head()

Prediction_Matrix_Test = pd.DataFrame(Predict_test, index = Pivot_Data_test.index.values.tolist(), columns = Pivot_Data_test.my_score.columns.values.tolist())
Prediction_Matrix_Test.head()

col_test = list(Pivot_Data_Normalized_test.my_score.columns.values)
ind_test = list(Pivot_Data_Normalized_test.index)
data_test = Pivot_Data_Normalized_test.values
Rating_Real_test = pd.DataFrame(data_test, index=ind_test, columns=col_test)
Rating_Real_test.head()

"""Scaling to 0~1 for easier analysis"""

Rating_Real_test_Scaled = (Rating_Real_test - Rating_Real_test.min())/(Rating_Real_test.max() - Rating_Real_test.min())

Prediction_Matrix_Test_Scaled = (Prediction_Matrix_Test - Prediction_Matrix_Test.min())/(Prediction_Matrix_Test.max() - Prediction_Matrix_Test.min())

Prediction_Matrix_Sparsed_Test = Prediction_Matrix_Test_Scaled + Rating_Real_test - Rating_Real_test

Rating_Couple_Test = Rating_Real_test_Scaled.T.values.copy()
Rating_Real_NP_Test = Rating_Real_test_Scaled.fillna(value=0).T.values.copy()
Rating_Matrix_NP_Test = Prediction_Matrix_Test_Scaled.T.values.copy()

Rating_Couple_Test = Rating_Couple_Test.tolist()

for i in range(len(Rating_Real_NP_Test)):
  for j in range(len(Rating_Real_NP_Test[i])):
    Rating_Couple_Test[i][j] = (Rating_Real_NP_Test[i][j], Rating_Matrix_NP_Test[i][j])

"""Top K Recommendations"""

recommended = 0
relevant = 0
true_pos = 0

true_pos_list = []
rec_list = []
rel_list = []

for i in range(len(Rating_Couple_Test)):
  Rating_Couple_Test[i].sort(key=lambda x: x[0], reverse=True)
  K_Relevant = Rating_Couple_Test[i][0:10]

  true_pos_i = 0
  rec_i = 0
  rel_i = 0

  for couple in K_Relevant:
    if couple[0] >= 0.5:
      rel_i = rel_i + 1
      relevant = relevant + 1
    if couple[1] >= 0.5:
      rec_i = rec_i + 1
      recommended = recommended + 1
    if couple[0] >= 0.5 and couple[1] >= 0.5:
      true_pos_i = true_pos_i + 1
      true_pos = true_pos + 1 
    
  true_pos_list.append(true_pos_i)
  rec_list.append(rec_i)
  rel_list.append(rel_i)

true_pos, recommended, relevant

"""We got almost 99% precision for the users' top 10 animes, which is pretty good, even though we used only 1500 users to estimate our anime features. In a real situation we would have used a bigger sample if not all dataset, but for this example we got a good enough result."""

precision_K = true_pos/recommended
recall_K = true_pos/relevant

precision_K, recall_K

"""For each person"""

precision_i = np.zeros(300)
recalls_i = np.zeros(300)

for i in range(len(true_pos_list)):
  precision_i[i] = true_pos_list[i]/rec_list[i] if rec_list[i] != 0 else 1
  recalls_i[i] = true_pos_list[i]/rel_list[i] if rel_list[i] != 0 else 1

prec_mean = sum(precision_i)/len(precision_i)
rec_mean = sum(recalls_i)/len(recalls_i)

prec_mean, rec_mean

"""For all Predictions:"""

true_pos = 0
relevant = 0
recommended = 0

for i in range(len(Prediction_Matrix_Sparsed_Test.values)):
  for j in range(len(Prediction_Matrix_Sparsed_Test.values[i])):
    if Prediction_Matrix_Sparsed_Test.values[i][j] >= 0.5 and Rating_Real_test_Scaled.values[i][j] >= 0.5:
      true_pos = true_pos + 1
   
    if Rating_Real_test_Scaled.values[i][j] >= 0.5:
      relevant = relevant+1
    
    if Prediction_Matrix_Sparsed_Test.values[i][j] >= 0.5:
      recommended = recommended + 1

true_pos, relevant, recommended

true_pos/recommended, true_pos/relevant

"""# Using the model from collaborative to make recommendation to a User"""

User_Anime = User_data[['anime_id', 'title', 'user_rating']].copy()

User_Anime.dropna(inplace=True)
User_Anime

User_Anime_Pivot = pd.DataFrame(User_Anime.user_rating.values, index = User_Anime.title.values.tolist(), columns = ['user_rating'])
User_Anime_Pivot_Normalized = User_Anime_Pivot - User_Anime_Pivot.mean()

User_Anime_Pivot_Normalized

J_list = []
lambda_reg = 0.001
alpha = 0.0008

User_Coeff = np.random.rand(10,1)
User_Anime_Pivot;

Anime_Watched_Features = anime_features_svd[anime_features_svd.index.isin(User_Anime.title.tolist())]

Anime_Watched_Features = Anime_Watched_Features.values @ s_matrix

m = len(Anime_Watched_Features)

J_Cost_temp = 10**20

for i in range(25000):
  Rating_Predict_User = Anime_Watched_Features @ User_Coeff
  Reg_Term = (lambda_reg/(2*m))*(((User_Coeff**2).sum(axis=1)).sum())
  
  J_Cost = (1/(2*m))*(((Rating_Predict_User - User_Anime_Pivot_Normalized.values)**2).sum().sum()) + Reg_Term
  J_Cost_temp = J_Cost
  J_list.append(J_Cost)

  User_Coeff_temp = (alpha/m)*((((Rating_Predict_User - User_Anime_Pivot_Normalized.values)).transpose()).dot(Anime_Watched_Features))
  User_Coeff = (1-lambda_reg*alpha/m)*User_Coeff - User_Coeff_temp.transpose()

Anime_Recommend_User = pd.DataFrame(((anime_features_svd.values @ s_matrix) @ User_Coeff), index=anime_features_svd.index.tolist(), columns=['Recommendation Rating'])

Anime_Recommend_User = Anime_Recommend_User[~Anime_Recommend_User.index.isin(User_data.title.tolist())]

"""Scaling the range of ratings to be between 0 and 1"""

Anime_Recommend_User = (Anime_Recommend_User - Anime_Recommend_User.min()[0])/(Anime_Recommend_User.max()[0] - Anime_Recommend_User.min()[0]) # Just to show values as < 1, but not necessary

Anime_Recommend_User.sort_values(by='Recommendation Rating', ascending=False)[:20]

Anime_Recommend_User.loc['R.O.D the TV']

"""I also scaled the values obtained in the content based step to have a range of 0~1"""

Recommend_Content = Recommend[Recommend.index.isin(anime_features_svd.index.tolist())]
Recommend_Content_DF = pd.DataFrame(Recommend_Content.values, index=Recommend_Content.index.tolist(), columns=['Recommendation Rating'])
Recommend_Content_DF = Recommend_Content_DF.reindex(Anime_Recommend_User.index)

Recommend_Content_DF = (Recommend_Content_DF - Recommend_Content_DF.min())/(Recommend_Content_DF.max() - Recommend_Content_DF.min()) 

Recommend_Content_DF.sort_values(by='Recommendation Rating', ascending=False)[:20]

"""This is the final result, where we have a good list of recommendations for the user based both in his content preferences and in his past animes ratings. I used a list of most action/adventure animes that I have watched so I could see if the final result would make sense, and it seems that the system worked as intended.

I scaled the recommendation ratings of both steps and then got the average between the two values of each anime to get a hybrid recommendation rating. I also though of using a the value from the content based step as a weight, and use it to multiply by the recommendation rating from the collaborative step. I chose to use the average approach, though, since it gives equal weight to both systems.
"""

Hybrid_Recommendation = (Anime_Recommend_User + Recommend_Content_DF)/2
Hybrid_Recommendation.sort_values(by='Recommendation Rating', ascending=False)[:40]

